{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "helpers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNDxS3rbz37J4eC3UfJGlN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RIelpuzpXwY"
      },
      "source": [
        "def plot_roc_PD(df_preds_label, df_y_val, str_s):\n",
        "  \"\"\"\n",
        "  plot ROC Chart on the Probability of Default\n",
        "  args: \n",
        "    df_preds_label: predicted results\n",
        "    df_y_val: true labels \n",
        "    str_s: plot legend\n",
        "  returns: \n",
        "    None\n",
        "  \"\"\"\n",
        "  fallout, sensitivity, thresholds = roc_curve(df_y_val, df_preds_label)\n",
        "  plt.plot(fallout, sensitivity, label='%s' % str_s)\n",
        "  plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "  plt.title(\"ROC Chart on the Probability of Default\")\n",
        "  plt.xlabel('Fall-out')\n",
        "  plt.ylabel('Sensitivity')\n",
        "  plt.legend()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37nMXk64oatA"
      },
      "source": [
        "def plot_prob_thresh(df_preds, df_y_val):\n",
        "  \"\"\"\n",
        "  plot probability threshold for Good recall, bad recall, good accuracy and bad accuracy\n",
        "  for selecting proper threshold \n",
        "  args: \n",
        "    df_preds: predicted results\n",
        "    df_y_val: true labels \n",
        "  returns: \n",
        "    None\n",
        "  \"\"\"\n",
        "  arr_thresh = np.arange(0, 1, 0.1)\n",
        "  arr_good_recall = np.zeros(arr_thresh.shape)\n",
        "  arr_bad_recall = np.zeros(arr_thresh.shape)\n",
        "  arr_good_accuracy = np.zeros(arr_thresh.shape)\n",
        "  arr_bad_accuracy = np.zeros(arr_thresh.shape)\n",
        "\n",
        "  for index in np.arange(len(arr_thresh)):\n",
        "    df_preds['Risk_pred'] = df_preds['prob_default'].apply(lambda x: 1 if x > arr_thresh[index] else 0)\n",
        "    arr_good_recall[index] = precision_recall_fscore_support(df_y_val, df_preds['Risk_pred'])[1][0]\n",
        "    arr_bad_recall[index] = precision_recall_fscore_support(df_y_val, df_preds['Risk_pred'])[1][1]\n",
        "    arr_good_accuracy[index] = precision_recall_fscore_support(df_y_val, df_preds['Risk_pred'])[0][0]\n",
        "    arr_bad_accuracy[index] = precision_recall_fscore_support(df_y_val, df_preds['Risk_pred'])[0][1]\n",
        "\n",
        "  plt.plot(arr_thresh, arr_good_recall)\n",
        "  plt.plot(arr_thresh, arr_bad_recall)\n",
        "  plt.plot(arr_thresh, arr_good_accuracy)\n",
        "  plt.plot(arr_thresh, arr_bad_accuracy)\n",
        "  plt.xlabel(\"Probability Threshold\")\n",
        "  plt.legend([\"Good Recall\", \"Bad Recall\", \"Good Accuracy\", \"Bad Accuracy\"])"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}